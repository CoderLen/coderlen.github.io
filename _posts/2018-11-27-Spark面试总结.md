---
layout:     post
title:      "Spark面试总结"
author:     "CoderLen"
header-img: "img/in-post/bigdata.jpg"
tags:
    - 面试
    - 大数据
    - Spark
---

## Spark面试总结

1. RDD是什么？有什么特性？

   RDD是弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面元素可并行计算的集合。弹性表示RDD中的数据可以存储在内存或者是磁盘，RDD的分区是可以改变的。

   五大特性：

    	1. 一个分区列表，RDD中数据都存在一个分区列表里面；
    	2. 作用在每一个分区中的函数
    	3. 一个RDD依赖于其他RDD，也就是血统
    	4. 可选的，针对kv类型的RDD才有的特性
    	5. 可选的，RDD的每个分区在计算时会选择最佳的计算位置；

2. RDD有哪些缺陷？

   1. 不支持细粒度的写和更新操作(如网络爬虫)，spark写数据是粗粒度的，所谓粗粒度就是批量写入数据。
   2. 不支持增量迭代计算，Flink支持；

3. Spark怎么划分Job，Stage, Task?

   Spark按action算子划分Job，例如count(), foreach()，reduce(), collect()等；

   按宽依赖/action划分stage，有ShuffleStage，ResultStage

   task是spark的最小执行单元，跟partition有关。

4. 宽依赖，窄依赖

   - 宽依赖：指多个子RDD的Partition会依赖同一个父RDD的Partition
   - 窄依赖：每一个父RDD的Partition最多被子RDD的一个Partition使用；

5. cache()和persist()方法的区别？

   1. cache和persist都是用于将一个RDD进行缓存，这样在之后的使用就不需要重新计算了，可以节省程序运行时间。
   2. cache()内部调用了persist()，默认缓存级别是MEMORY_ONLY。而persist()可以根据情况设置其它的缓存级别；
   3. 

6. cache()后面能不能接其它算子，它是不是action操作；

   cache可以接其它算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。cache不是action算子。

7. repartition()和coalesce()的区别？

8. hadoop和spark的shuffle的相同之处和差异？

   |                        | MapReduce                                         | Spark                                                        |
   | ---------------------- | ------------------------------------------------- | ------------------------------------------------------------ |
   | collect                | 在内存中构造了一块数据结构用于map输出的缓冲       | 没有在内存中构造一块数据结构用于map输出的缓冲，而是直接把输出写到磁盘文件 |
   | sort                   | map输出的数据有排序                               | map输出的数据没有排序                                        |
   | merge                  | 对磁盘上的多个spill文件最后进行合并成一个输出文件 | 在map端没有merge过程，在输出时直接是对应一个reduce的数据写到一个文件中，这些文件同时存在并发写，最后不需要合并成一个 |
   | copy框架               | jetty                                             | netty或者直接socket流                                        |
   | 对于本节点上的文件     | 仍然是通过网络框架拖取数据                        | 不通过网络框架，对于在本节点上的map输出文件，采用本地读取的方式 |
   | copy过来的数据存放位置 | 先放在内存，内存放不下时写到磁盘                  | 一种方式全部放在内存；另一种方式先放在内存                   |
   | merge sort             | 最后会对磁盘文件和内存中的数据进行合并排序        | 对于采用另一种方式时也会有合并排序的过程                     |

9. cogroup rdd的实现原理，在什么场景下用过这个rdd？

   cogroup将多个RDD中同一个Key对应的Value组合到一起。最多可以组合四个RDD。

10. spark master ha主从切换的过程不会影响集群已有的作业运行，为什么？

    因为程序在运行之前，已经申请过资源了，剩下就是driver和executors通讯，不需要和master通讯。

11. driver的功能是什么？

    1. 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext实例，是程序的入口点；
    2. 负责向集群申请资源，向master注册信息，负责作业的调度，负责作业的解析，生成stage并调度task到executor上，包括DAGScheduler，TaskScheduler;

12. spark中，worker的主要作用是什么？

    管理当前节点内存，CPU资源，接收master分配过来的资源指令，通过ExecutorRunner启动程序分配任务。需要注意的是：

    1. worker会不会汇报当前信息给master，worker心跳给master主要有workid，它不会发送资源信息以心跳的方式给master，master分配的时候就知道worker，只有出现故障时才会发送资源。
    2. worker不会运行代码，具体运行的是Executor。

13. Spark算子（map，flatmap，reduceByKey和reduce，groupByKey和reduceByKey，join，distinct）的原理。

14. Spark Task资源分配，任务调度，master计算资源分配。

    - 资源调度

      1. Driver调度（分配Driver执行容器，1个）

         Master中调度程序执行时，会为Driver分配一个满足其执行要求的Worker，并通知Worker启动该Worker。DriverRunner执行Driver（mainclass会创建SparkContext，SparkContext会创建DAGSchedueler和TaskScheduler分别用于Stage调度和任务调度，并会触发RDD的action算子提交job）

      2. App调度（分配Executor，多个）

         Driver成功执行后，会通过SparkDeployScheduler创建AppClient，用于向Master汇报资源需求。Master接到Appclient汇报后，将其加入waitting队列，等待调度。

         App调度时会为app分配满足条件的资源-worker，然后worker启动executor，及向AppClient发送ExecutorAdd消息。

         进行调度时，会根据配置SpreadOutApps=spark.deploy.spreadOut情况决定资源分配方式。

         2.1. SpreadOutApps方式：将每个app分配到尽可能多的worker中执行。

          	1. 从列表中取下一个app，根据cpu情况找出合适的worker，按核数从小到大排序
          	2. 如果worker节点存在可以分配的core则进行预分配，并在分配列表中计数已经分配了多少core，assigned。
          	3. 根据assigned列表中的预分配信息，进行分配Executor(真实分配)。
          	4. 启动Executor并设置app.state=Application.RUNNING

         2.2. 非SpreadOutApps方式：将每一个app分配到尽可能少的worker上执行。

         1. 从可用的worker列表中去下一worker。（worker <- workers if worder.coresfree > 0）

         2. 遍历waittingApps找到满足app运行条件的app，进行分配。

         3. 启动Executor(lauchExecutor(w,e))，并设置app.state=Application.RUNNING

            其中：launchExecutor(worker,exec)具体内容是向executor分配worker，通知worker启动executor。


    - 任务Task调度
    
      当stage不存在缺失的ParentStage时，会将其转换为taskset并提交。转换时会根据stage类型进行转换：将ResultStage转换成ResultTask，ShuffleMapStage转换成ShuffleMapTask。Task个数由finalRDD中的分区数决定。
    
      当转换成的TaskSet提交后，将其通过TaskScheduler包装成TaskSetManager并添加到调度列表中（pool），等待调度。在包装成TaskSetManager时，根据task的preferredLocations将任务分类存放到pendingTaskWithExecutor,pendingTaskForHost，pendingTaskForRack，pendingTaskWithNoPrefs及allPendingTask中。
    
      在进行Task调度时，首先根据调度策略将可调度的所有taskset进行排序，然后对排好序的taskset待调度列表的taskset按序进行分配executor。在分配executor时，然后逐个为executor列表中可用的executor在此次选择的taskset中按本地性由高到低查找适配任务。
    
      当选定某一task后，将其放到runningtask列表，执行完后，放到success列表，下次调度时就会过滤这些任务，避免重复调度。
    
      当taskset待调度列表为空，即所有任务job执行完成。



    - 调度策略
    
      Spark现有的调度策略有FIFO及Fair两种。采用何种策略由“spark.scheduler.mode”参数指定。


​     

15. Spark SQL定义函数，怎么创建DataFrame

    - 读取json格式文件创建DataFrame

      dataframe是一个一个row类型的rdd，df.rdd()/df.javaRdd()

      ```scala
      //创建sqlContext
      SQLContext sqlContext = new SQLContext(sc);//SprakSQL中是SQLContext对象
              
      /**
       * DataFrame的底层是一个一个的RDD  RDD的泛型是Row类型。
       * 以下两种方式都可以读取json格式的文件
       */
       DataFrame df = sqlContext.read().format("json").load("sparksql/json");
      // DataFrame df2 = sqlContext.read().json("sparksql/json.txt");
      // df2.show();
       /**
        * DataFrame转换成RDD
        */
       RDD<Row> rdd = df.rdd();
      ```

    - 通过json格式的rdd创建dataframe

      ```scala
      val nameRDD = sc.makeRDD(Array(
        "{\"name\":\"zhangsan\",\"age\":18}",
        "{\"name\":\"lisi\",\"age\":19}",
        "{\"name\":\"wangwu\",\"age\":20}"
      ))
      val scoreRDD = sc.makeRDD(Array(
              "{\"name\":\"zhangsan\",\"score\":100}",
              "{\"name\":\"lisi\",\"score\":200}",
              "{\"name\":\"wangwu\",\"score\":300}"
              ))
      val nameDF = sqlContext.read.json(nameRDD)
      val scoreDF = sqlContext.read.json(scoreRDD)
      nameDF.registerTempTable("name")       
      scoreDF.registerTempTable("score")     
      ```

    - 非json格式的rdd创建dataframe

      - 通过反射的方式将非json的rdd转换成df（不建议使用）

      - d动态创建schema，将非json格式的rdd转换成df。

        ```scala
        /**
         * 动态构建DataFrame中的元数据，一般来说这里的字段可以来源自字符串，也可以来源于外部数据库
         */
        List<StructField> asList =Arrays.asList(//这里字段顺序一定要和上边对应起来
            DataTypes.createStructField("id", DataTypes.StringType, true),
            DataTypes.createStructField("name", DataTypes.StringType, true),
            DataTypes.createStructField("age", DataTypes.IntegerType, true)
        );
        
        StructType schema = DataTypes.createStructType(asList);
        DataFrame df = sqlContext.createDataFrame(rowRDD, schema);
        ```

      - 读取parquet文件创建dataframe

      - 读取jdbc中的数据创建dataframe

16. Spark Streaming项目多久一个批次数据。

17. Spark调优

18. Spark数据倾斜解决方案

    - 数据倾斜产生的原因

      - 读kafka时，用DirectStream方式读取Kafka数据时，由于Kafka的每一个Partition对应spark的一个task，所以kafka内相关topic的partition之间数据是否均衡，直接决定Spark处理该数据是否会产生数据倾斜。

      - 读文件时，Spark通过textFile(path, minPartition)方法读取文件时，使用的是TextFileFormat。对于不可切分文件来说（如压缩文件），每个文件对应一个Split，从而对应一个partition。所以一个文件大小是否一致，很大程度上决定了是否存在数据源侧的数据倾斜。对于可切分文件来说，每个split大小由如下算法决定。其中一个goalsize等于所有文件总大小除以minPartition。默认情况下，各split大小不会太大，一般相当于一个block大小，所以数据倾斜问题不明显，如果出现严重的数据倾斜，可通过上述的参数调整。

        ```
        computeSplitSize(long goalsize, long minsize, long blocksize){
            return Math.max(minSize, Math.min(goalsize, blocksize));
        }
        ```

    - 解决方案

      - 尽量使用可切分文件代替不可切分文件

      - 调整并行度分散同一个task的不同key

        需要shuffle的操作算子上直接设置并行度，或者使用spark.default.parallelism设置。如果是spark sql，还可以通过设置spark.sql.shuffle.paritions=[num_tasks]设置并行度。

      - 自定义partitioner，将原本分配到同一个task的不同key分配到不同task。

      - 将reduce side join转变成map side join

        通过spark的broadcast机制，将reduce侧join转换为map侧join，避免shuffle从而完全消除shuffle带来的数据倾斜。

      - 为skew的key增加随机前/后缀

      - 大表随机添加N中随机前缀，小表扩大N倍

19. Spark的内存管理机制，spark 1.6前后对比

    spark1.6之后引入的统一内存管理机制，与静态内存管理 的区别在于存储内存和执行内存共享同一块空间（统一内存），可以动态占用对方的空闲空间。

20. spark rdd，dataframe，dataset区别

    RDD不支持sparksql操作

    dataframe每一行的类型固定为ROW，只有通过解析才能获取到各个字段的值。

    dataset与dataframe均支持sparksql操作，还是注册临时表/视窗，进行sql语句操作

    dataframe也可以叫做dataset[Row]，每一行的类型是Row，不解析。

    dataset中，每一行是什么类型是不一定的，在自定义case class后可以很自由地获取每一行的信息。

    Dataset可以认为是DataFrame的一个特例，主要区别是Dataset每一个records存储的是一个强类型值而不是一个Row。因此具有如下三个特点：

    1. DataSet可以在编译时检查类型
    2. 并且是面向对象的编程接口
    3. 后面的DataFrame会继承DataSet，DataFrame是面向Spark SQL的接口。

21. Spark有哪几种join？

    join：类似sql中的inner join，关联不上的会被过滤掉

    leftOuterJoin：类似sql中的左外关联left out join，返回结果以第一个rdd为主，关联不上的记录为空

    rightOuterJoin：类似sql的右外连接right out join，返回结果以参数，也就是第二个rdd为主，关联不上的记录为空。

22. Spark JDBC(mysql)读取并发度优化？

    - 单partition（无并发）

      ```scala
      val jdbcDF = sqlContext.read.jdbc(url, tableName, prop)
      ```

    - 根据Long类型字段分区

      调用函数

      ```
        def jdbc(
        url: String,
        table: String,
        columnName: String,    # 根据该字段分区，需要为整形，比如id等
        lowerBound: Long,      # 分区的下界
        upperBound: Long,      # 分区的上界
        numPartitions: Int,    # 分区的个数
        connectionProperties: Properties): DataFrame
      ```

    - 根据任意类型字段分区

      调用函数

      ```
      jdbc(
        url: String,
        table: String,
        predicates: Array[String],
        connectionProperties: Properties): DataFrame
      ```

      predicates参数如下：

      ```
      val predicates =
          Array(
            "2015-09-16" -> "2015-09-30",
            "2015-10-01" -> "2015-10-15",
            "2015-10-16" -> "2015-10-31",
            "2015-11-01" -> "2015-11-14",
            "2015-11-15" -> "2015-11-30",
            "2015-12-01" -> "2015-12-15"
          ).map {
            case (start, end) =>
              s"cast(modified_time as date) >= date '$start' " + s"AND cast(modified_time as date) <= date '$end'"
          }
      
      ```

23. Spark Join算子可以用什么代替？

    可以用cogroup算子代替，cogroup相当于SQL中的全外关联full outer join，返回左右RDD中的记录，关联不上的为空。

24. Spark Streaming是怎么跟Kafka交互的，具体的代码怎么写？程序的执行流程是怎样的，这个过程怎么确保数据不丢失（直连和receiver模式）

    - 怎么跟Kafka交互

      - receiver模式

        <b>原理</b>：是基于kafka的high-level api来实现的对kafka数据的消费的。

        在提交spark streaming任务后，spark集群会划出指定的receiver来专门持续不断，异步读取kafka数据，读取的时间间隔以及每次读取offset范围可以由参数指定。读取的数据保存在receiver中，具体的StorageLevel方式由用户指定，例如MEMORY_ONLY等。当driver触发batch任务时，receivers中的数据会转移到剩余的executors中执行，执行完后receivers会相应的更新zookeeper的offset。如果确保at least once的读取方式，可以设置spark.streaming.receiver.writeAheadLoag.enable为true。

        ![](https://github.com/stevekangpei/hello-world/blob/master/Spark-Streaming-Kafka_ReceiverBased.jpeg?raw=true)

        Kafka的high-level数据读取方式让用户可以专注于所读数据，而不用关注或维护consumer的offset，这减少了用户的工作量及代码量而且相对比较简单。

        具体代码如下

        ![](https://github.com/stevekangpei/hello-world/blob/master/Spark-Streaming-Kafka_Recerverbased_Implement.jpeg?raw=true)

        <b>receiver模式读取问题</b>

        1. 优化：

           1. 防止数据丢失。做Checkpoint操作以及配置spark.streaming.receiver.writeAheadLog.enable参数
           2. 提高receiver数据吞吐量。采用MEMORY_AND_DISK_SER方式读取数据，提高单receiver的内存或者调大并行度，将数据分散到多个receiver中去。

        2. 可能出现的问题

           1. 配置spark.streaming.receiver.writeAheadLog.enable参数，每次处理之前需要将该batch内的日志备份到Checkpoint目录中，这降低了数据处理效率，反过来又加重了receiver端的压力。另外由于数据备份机制，会受到负载影响，负载一高就会出现延迟的风险，导致应用崩溃。
           2. 采用MEMORY_AND_DISK_SER降低了对内存的要求，但在一定程度上影响了计算的速度。
           3. 单receiver内存。由于receiver也是属于executor的一部分，那么为了提高吞吐量，提高receiver内存。但是在每次的batch计算中，参与计算的batch并不会使用那么多的内存，导致资源浪费严重。
           4. 提高并行度。采用多个receiver来保存kafka的数据。receiver读取数据是异步的，并不参与计算。如果开较高的并行度来平衡吞吐量不是很划算。
           5. receiver和计算的executor是异步的，那么遇到网络等因素影响，导致计算出现延迟，计算队列一致在增加，而receiver则一致接收数据，这非常容易导致程序崩溃。
           6. 在程序失败恢复的时候，有可能出现数据部分落地，但是程序失败，为更新offsets的情况，这导致数据重复消费。

      - direct模式

        spark在1.3版本引入了Direct方式消费kafka数据。相对于receiver模式，direct模式具有以下方面的优势：

        1. 简化并行。不再需要创建挤一挤多个union多输入源，kafka topic的partition与rdd的partition一一对应。
        2. 高效。receiver模式保证数据零丢失（zero-data loss）需要配置spark.streaming.receiver.writeAheadLog.enable，这种方式需要保存两份数据，浪费存储空间，也影响效率。而direct模式不存在这个问题。
        3. 强一致语义。High-Level数据由spark streaming消费，但是offsets则是由zookeeper保存。通过参数配置，可以实现at-least once消费，此种情况有重复消费数据的可能。

        Direct方式采用kafka简单的consumer api方式读取数据，无需经由zookeeper，这种方式也不再需要专门的receiver来持续不断的读取数据。当batch任务触发时，由executor读取数据，并参与到其他Executor的数据计算过程中去。Driver来决定读取多少offsets，并将offsets交由checkpoints来维护。将触发下次batch任务，再由executor读取kafka数据并计算。direct模式在需要计算时再读取数据，所以对内存的要求不高，只需要考虑批量计算所需要的内存即可，另外batch任务堆积时，也不会影响数据堆积。

        <b>不足</b>

        1. 提高了成本。需要用户采用checkpoint或者第三方存储来维护offset是，而不像receiver模式那样，通过zk来维护offset是，此提高了开发成本。
        2. 监控可视化。receiver模式指定topic指定consumer的消费情况均能通过zk来监控。direct没有这种便利，则需要投入人力开发。

25. Spark运行流程，源码架构

26. Spark Standalone模型和Yarn架构模型的架构图

27. Spark Streaming的反压机制

    Spark的反压机制是1.5之后的版本才引入的，只需要配置spark.streaming.backpressure.enable为true即可，反压机制涉及以下几个参数：

    - spark.streaming.backpressure.initialRate: 启用反压机制每一个接收器接收第一批数据的初始最大速率。
    - spark.streaming.backpressure.pid.minRate：可以估算的最低费率是多少。默认是100，只能设为非负值
    - 如果用户还配置了spark.streaming.receiver.maxRate或spark.streaming.kafka.maxRatePerPartition,那么最后到底接收多少数据取决于三者的最小值。也就是说每个接收器或者每个kafka分区每秒处理的数据不会超过这两个值。

    为了实现自动调节数据的传输速率，新增了RatController组件，这个组件继承自StreamingListener，其监听所有作业的onBatchCompleted事件，并且基于processingDelay，schedulingDelay，当前Batch处理的记录条数以及处理完成事件来估算出一个速率，这个速率主要用于更新流每秒能处理的最大记录数。速率估算器RateEstimator可以多种实现。



### 参考

1. <a href="http://www.cnblogs.com/jxhd1/p/6528540.html">Spark的Shuffle过程介绍</a>
2. http://www.cnblogs.com/barrenlake/p/4550800.html
3. <a href='https://blog.csdn.net/stevekangpei/article/details/76995393'>Spark-Streaming 和Kafka连接的两种方式</a>